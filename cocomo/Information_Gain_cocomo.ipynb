{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWVNPNDz_Gu2",
        "outputId": "89d30c8f-94e2-477e-edff-745bfbb7cf42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "==============================\n",
            "information gain\n",
            "{'1': 0.045370512395678464, '5': 0.045182293746151014, '11': 0.04463990360528669, '2': 0.044287041693371165, '15': 0.04410990137510762, '6': 0.043747231474580595, '4': 0.043558161414487984, '12': 0.043360924542535706, '16': 0.042929158026844494, '7': 0.042046334889543235, '10': 0.041642856003507145, '9': 0.041553004391484016, '0': 0.04138243845446432, '3': 0.04138243845446432, '8': 0.04067306818926486, '13': 0.04059059592643699, '14': 0.040416234772919335}\n",
            "==============================\n",
            "==============================\n",
            "top 10 features\n",
            "[1, 5, 11, 2, 15, 6, 4, 12, 16, 7]\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "import operator\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split as split_data\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class FeatureSelection:\n",
        "\n",
        "    def __init__(self, csv, num_feature_select):\n",
        "\n",
        "        self.information_gain = {}  # Information gain for all features numbered 0 - (n - 1)\n",
        "        self.num_feature_select = num_feature_select  # Number of top features to select\n",
        "        self.top_n_features = []  # Top n features\n",
        "\n",
        "         \n",
        "        # reading the data and setting the header\n",
        "        self.csv_data = pd.read_csv(csv, header=1)\n",
        "\n",
        "        # separation of input and output - X, Y\n",
        "        self.X = self.csv_data.iloc[:, :-1]\n",
        "        self.X = scaler.fit_transform(self.X) #  Standardized using Normal Distribution \n",
        "\n",
        "        self.Y = self.csv_data.iloc[:, -1]\n",
        "\n",
        "\n",
        "    #expected information gain\n",
        "    def exp_IG(self):\n",
        "        x = self.X #input\n",
        "        y = self.Y.values[:] #output\n",
        "\n",
        "        #calculation of entropy pre-defined formula\n",
        "        def _entropy(values): #initial input is y which is output\n",
        "            uniqw, inverse = np.unique(values, return_inverse=True) \n",
        "            counts = np.bincount(inverse)\n",
        "            probs = counts[np.flatnonzero(counts)] / float(len(values))\n",
        "            # print(1 - probs, probs)\n",
        "            return np.sum(probs * np.exp(1 - probs))\n",
        "\n",
        "        # calc updated entropy for given input x, output y\n",
        "        def ig(feature, y):\n",
        "            feature_set_indices=[]\n",
        "            feature_not_set_indices=[]\n",
        "            for i in range(len(feature)):\n",
        "                if feature[i] >=0 :\n",
        "                    feature_set_indices.append(i)\n",
        "                else :\n",
        "                    feature_not_set_indices.append(i)\n",
        "            # print(feature_set_indices)\n",
        "            # print(feature_not_set_indices)\n",
        "            # feature_set_indices = np.nonzero(feature) #find index of non zero elements index as tuple with x, y\n",
        "            # print(feature_set_indices)\n",
        "            # feature_not_set_indices = [i for i in feature_range if i not in feature_set_indices[0]] # find the zero element x position index\n",
        "            # print(feature_not_set_indices)\n",
        "            entropy_x_set = _entropy(y[feature_set_indices]) #calc entropy for non zero elements\n",
        "            entropy_x_not_set = _entropy(y[feature_not_set_indices]) #calc entropy for zero elements\n",
        "\n",
        "            #return the updated entropy\n",
        "            return entropy_before - (((len(feature_set_indices) / float(feature_size)) * entropy_x_set)\n",
        "                                     + ((len(feature_not_set_indices) / float(feature_size)) * entropy_x_not_set))\n",
        "\n",
        "        feature_size = x.shape[0] #number of features\n",
        "        feature_range = range(0, feature_size) #possible number of features\n",
        "        entropy_before = _entropy(y)\n",
        "        # print(entropy_before)\n",
        "        information_gain_scores = [] # assuming that the elements score are in the indexed order of the input features\n",
        "        # print(x.T.shape)\n",
        "        for feature in x.T: #for each column in input\n",
        "            # print(feature)\n",
        "            information_gain_scores.append(ig(feature, y)) #find the entropy of each feature x , store it\n",
        "        # print(information_gain_scores)\n",
        "        \n",
        "        info_gain = {} \n",
        "\n",
        "        for i in range(self.X.shape[1]): #total number of features\n",
        "            # feature index = its entropy/information-gain-score\n",
        "            info_gain[str(i)] = information_gain_scores[i]\n",
        "\n",
        "        #sorted infromation about features\n",
        "        # print(\"==============================\")\n",
        "        # print(\"before sort\")\n",
        "        # print(info_gain)\n",
        "        # print(\"==============================\")\n",
        "        info_gain = sorted(info_gain.items(), key=operator.itemgetter(1), reverse=True)\n",
        "        # print(\"==============================\")\n",
        "        # print(\"after sort\")\n",
        "        # print(info_gain)\n",
        "        # print(\"==============================\")\n",
        "\n",
        "\n",
        "        #for each feature column\n",
        "        for i in range(self.X.shape[1]):\n",
        "            if i < self.num_feature_select: #if i is less than the required number of features that we need\n",
        "                self.top_n_features.append(int(info_gain[i][0])) # append feature information to list we require about top feautres\n",
        "            self.information_gain[info_gain[i][0]] = info_gain[i][1]\n",
        "\n",
        "        # return information_gain_scores, []\n",
        "\n",
        "    def mutual_info_calculator(self):\n",
        "        information_gain = []\n",
        "        information_gain.append(mutual_info_regression(self.X, self.Y, discrete_features=self.discrete_features))\n",
        "\n",
        "        info_gain = {}\n",
        "\n",
        "        for i in range(self.X.shape[1]):\n",
        "            info_gain[str(i)] = information_gain[0][i]\n",
        "\n",
        "        info_gain = sorted(info_gain.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "        #for each feature column\n",
        "        for i in range(self.X.shape[1]):\n",
        "            if i < self.num_feature_select:\n",
        "                self.top_n_features.append(int(info_gain[i][0]))\n",
        "            self.information_gain[info_gain[i][0]] = info_gain[i][1]\n",
        "\n",
        "\n",
        "p = FeatureSelection('cocomo.csv', 10)\n",
        "\n",
        "# import pandas as pd\n",
        "# print(dtype(dataset))\n",
        "\n",
        "p.exp_IG()\n",
        "print(\"==============================\")\n",
        "print(\"==============================\")\n",
        "print(\"information gain\")\n",
        "print(p.information_gain)\n",
        "print(\"==============================\")\n",
        "print(\"==============================\")\n",
        "print(\"top 10 features\")\n",
        "print(p.top_n_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVLYAW0RfHgc",
        "outputId": "9a7c3830-1b55-4ebb-d4c1-f8c93d3915c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error : 0.8787794667110665\n"
          ]
        }
      ],
      "source": [
        "# Linear Regression \n",
        "x_data=p.X[:, p.top_n_features]\n",
        "y_data=p.Y \n",
        "x_train, x_test, y_train, y_test = split_data( x_data, y_data, test_size = 1/4, random_state = 80, shuffle=True) \n",
        "regressor = LinearRegression()\n",
        "regressor = regressor.fit(x_train, y_train)\n",
        "y_pred = regressor.predict(x_test)\n",
        "# Root Mean Sqaured Error\n",
        "#MSE = np.square(np.subtract(y_test,y_pred)).mean() \n",
        "\n",
        "MSE = np.square(np.subtract(y_test,y_pred)/np.maximum(np.absolute(y_test),np.absolute(y_pred))).mean() \n",
        "# print(MSE)\n",
        "RMSE = math.sqrt(MSE)\n",
        "print(\"Root Mean Squared Error :\",RMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzf0fMEDgPM6",
        "outputId": "bcd7613f-ff3e-4088-8c22-b79a78a55b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE error is 0.0798405588783106\n"
          ]
        }
      ],
      "source": [
        "# Mean Absolute Error\n",
        "from sklearn.metrics import mean_absolute_error as MAE\n",
        "#error = MAE(y_test, y_pred)\n",
        "error = (np.subtract(y_test,y_pred)/np.maximum(np.absolute(y_test),np.absolute(y_pred))).mean()\n",
        "print(f'MAE error is {error}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbFfDVfQg0qI",
        "outputId": "0571ad47-f504-4989-9545-307fa730ed5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Square Error:\n",
            "\n",
            "0.8112363883626443\n",
            "MAE error is -0.14391242208394053\n"
          ]
        }
      ],
      "source": [
        "# SVM\n",
        "from sklearn.model_selection import train_test_split as split_data\n",
        "X_train, X_test, Y_train, Y_test = split_data(x_data, y_data , test_size = 0.2 , random_state = 43, shuffle=True)\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import accuracy_score\n",
        "svr=SVR() \n",
        "svr.fit(X_train,Y_train)\n",
        "Y_pred=svr.predict(X_test)\n",
        "\n",
        "#Root Mean Squared Error\n",
        "MSE = np.square(np.subtract(Y_test,Y_pred)/np.maximum(np.absolute(Y_test),np.absolute(Y_pred))).mean() \n",
        "RMSE = math.sqrt(MSE)\n",
        "print(\"Root Mean Square Error:\\n\")\n",
        "print(RMSE)\n",
        "\n",
        "# Mean Absolute Error\n",
        "from sklearn.metrics import mean_absolute_error as MAE\n",
        "error = (np.subtract(Y_test,Y_pred)/np.maximum(np.absolute(Y_test),np.absolute(Y_pred))).mean()\n",
        "print(f'MAE error is {error}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD_lumaHizAQ",
        "outputId": "1d817141-3f70-469f-bb8b-f21b58ff6f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE error is -0.4672886257788695\n",
            "Root Mean Square Error:\n",
            "0.6730665237848918\n"
          ]
        }
      ],
      "source": [
        "# Random Forest\n",
        "from sklearn.model_selection import train_test_split as split_data\n",
        "x_train, x_test, y_train, y_test = split_data(x_data, y_data, test_size=0.20, shuffle=True)\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "treeModel = DecisionTreeRegressor(max_depth=5, random_state=None)\n",
        "treeModel.fit(x_train, y_train)\n",
        "model = RandomForestRegressor(max_depth=5, random_state=None,max_features='auto',max_leaf_nodes=5,n_estimators=50, criterion=\"absolute_error\")\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Mean Absolute Error\n",
        "from sklearn.metrics import mean_absolute_error as MAE\n",
        "error = (np.subtract(y_test,y_pred)/np.maximum(np.absolute(y_test),np.absolute(y_pred))).mean()\n",
        "print(f'MAE error is {error}')\n",
        "\n",
        "# Root Mean Squared Error\n",
        "MSE = np.square(np.subtract(y_test,y_pred)/np.maximum(np.absolute(y_test),np.absolute(y_pred))).mean() \n",
        "RMSE = math.sqrt(MSE)\n",
        "print(\"Root Mean Square Error:\")\n",
        "print(RMSE)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 32-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "9363dc131ef194b2f28a3b136b4153138bb891634d04c7d3fc6a2887135a1464"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}